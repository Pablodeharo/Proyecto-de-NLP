{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  is_spam\n",
      "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
      "1                             https://www.hvper.com/     True\n",
      "2                 https://briefingday.com/m/v4n3i4f3     True\n",
      "3   https://briefingday.com/n/20200618/m#commentform    False\n",
      "4                        https://briefingday.com/fan     True\n"
     ]
    }
   ],
   "source": [
    "# URL del conjunto de datos\n",
    "url_dataset = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\"\n",
    "\n",
    "# Cargar el conjunto de datos en un DataFrame de pandas\n",
    "df = pd.read_csv(url_dataset)\n",
    "\n",
    "# Visualizar las primeras filas del DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de entrenamiento:\n",
      "1569    [http, www, morningbrew, com, daily, story, 20...\n",
      "2229    [http, www, morningbrew, com, daily, story, 20...\n",
      "2296    [http, www, nytimes, com, article, maskne, acn...\n",
      "1800    [http, podcasts, apple, com, u, podcast, foxy,...\n",
      "1273    [http, www, nycpride, org, event, nyc, pride, ...\n",
      "Name: processed_url, dtype: object\n",
      "\n",
      "Conjunto de prueba:\n",
      "1376      [http, link, morningbrew, com, manage, 5z8, oc]\n",
      "932     [http, www, sciencedaily, com, release, 2019, ...\n",
      "144     [http, www, ft, com, content, eae603a4, a369, ...\n",
      "1752                        [http, thehustle, co, career]\n",
      "51      [http, www, caltech, edu, news, natural, fluid...\n",
      "Name: processed_url, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Preprocesamiento de URLs\n",
    "\n",
    "def preprocess_url(url):\n",
    "    url_parts = re.findall(r'\\b\\w+\\b', url)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    processed_url = [lemmatizer.lemmatize(word) for word in url_parts if word.lower() not in stop_words]\n",
    "    return processed_url\n",
    "\n",
    "df['processed_url'] = df['url'].apply(preprocess_url)\n",
    "\n",
    "# División del conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['processed_url'], df['is_spam'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualizar las primeras filas del conjunto de entrenamiento y prueba\n",
    "print(\"Conjunto de entrenamiento:\")\n",
    "print(X_train.head())\n",
    "print(\"\\nConjunto de prueba:\")\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados del clasificador SVM con parámetros por defecto:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.94      0.99      0.97       455\n",
      "        True       0.97      0.81      0.88       145\n",
      "\n",
      "    accuracy                           0.95       600\n",
      "   macro avg       0.96      0.90      0.92       600\n",
      "weighted avg       0.95      0.95      0.95       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define una función para tokenizar las URLs\n",
    "def tokenizar_url(url):\n",
    "    return url\n",
    "\n",
    "# Crear un pipeline con un vectorizador CountVectorizer y un clasificador SVM\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(analyzer=lambda x: x)), # Convertir la lista de tokens en una cadena para CountVectorizer\n",
    "    ('classifier', SVC()) # Clasificador SVM con parámetros por defecto\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predecir las etiquetas en el conjunto de prueba\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Analizar los resultados\n",
    "print(\"Resultados del clasificador SVM con parámetros por defecto:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiperparámetros encontrados: {'classifier__C': 10, 'classifier__gamma': 'scale', 'classifier__kernel': 'rbf'}\n",
      "Mejor puntuación de validación cruzada: 0.9545728949199722\n",
      "Resultados del mejor modelo:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.98      0.98      0.98       455\n",
      "        True       0.92      0.92      0.92       145\n",
      "\n",
      "    accuracy                           0.96       600\n",
      "   macro avg       0.95      0.95      0.95       600\n",
      "weighted avg       0.96      0.96      0.96       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define una función para tokenizar las URLs\n",
    "def tokenizar_url(url):\n",
    "    return url\n",
    "\n",
    "# Definir la cuadrícula de hiperparámetros a explorar\n",
    "param_grid = {\n",
    "    'classifier__C': [0.1, 1, 10, 100], # Parámetro de regularización\n",
    "    'classifier__kernel': ['linear', 'rbf', 'poly'], # Tipo de kernel\n",
    "    'classifier__gamma': ['scale', 'auto'], # Coeficiente gamma \n",
    "}\n",
    "\n",
    "# Crear un pipeline con CountVectorizer y SVC\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(analyzer=tokenizar_url)),\n",
    "    ('classifier', SVC())\n",
    "])\n",
    "\n",
    "# Inicializar GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Entrenar GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener los mejores hiperparámetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores hiperparámetros encontrados:\", best_params)\n",
    "\n",
    "# Obtener la mejor puntuación de validación cruzada\n",
    "best_score = grid_search.best_score_\n",
    "print(\"Mejor puntuación de validación cruzada:\", best_score)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluar el mejor modelo en el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"Resultados del mejor modelo:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo se ha guardado correctamente en: C:/Users/lenovo/Desktop/Proyectos Machine Learning/Bootcamp/Proyecto de NLP/Model/modelo_svm.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Especifica la ruta donde deseas guardar el modelo\n",
    "ruta_modelo = \"C:/Users/lenovo/Desktop/Proyectos Machine Learning/Bootcamp/Proyecto de NLP/Model/modelo_svm.joblib\"\n",
    "\n",
    "# Comprueba si la carpeta existe, si no, créala\n",
    "ruta_carpeta = os.path.dirname(ruta_modelo)\n",
    "if not os.path.exists(ruta_carpeta):\n",
    "    os.makedirs(ruta_carpeta)\n",
    "\n",
    "# Guarda el modelo en el archivo especificado\n",
    "joblib.dump(pipeline, ruta_modelo)\n",
    "\n",
    "print(\"El modelo se ha guardado correctamente en:\", ruta_modelo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
